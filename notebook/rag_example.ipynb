{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Retrieval-Augmented Generation (RAG) System",
   "id": "de50c9d4e13a4d66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dependencies installation",
   "id": "947c19c2452bbf21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install accelerate>=1.8.1\n",
    "!pip install docling>=2.41.0\n",
    "!pip install lancedb\n",
    "!pip install pandas\n",
    "!pip install sentence-transformers"
   ],
   "id": "28fdf8b545245055",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Parsing & Chunking",
   "id": "3e8e0d4735722b44"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "\n",
    "from docling import chunking\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class Chunker:\n",
    "    def __init__(self, embedding_model: str, max_tokens: int = 1024):\n",
    "        tokenizer = HuggingFaceTokenizer(\n",
    "            tokenizer=AutoTokenizer.from_pretrained(embedding_model),\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        self.__chunker = chunking.HybridChunker(tokenizer=tokenizer, merge_peers=True)\n",
    "\n",
    "    def chunk(self, source: str):\n",
    "        doc = DocumentConverter().convert(source=source).document\n",
    "        chunk_iter = self.__chunker.chunk(dl_doc=doc)\n",
    "        chunks = list(chunk_iter)\n",
    "        chunks_dicts = []\n",
    "        for chunk in chunks:\n",
    "            chunks_dicts.append(\n",
    "                {\n",
    "                    \"content\": chunk.text,\n",
    "                    \"page_number\": chunk.meta.doc_items[0].prov[0].page_no,\n",
    "                    \"pdf_name\": os.path.basename(source),\n",
    "                }\n",
    "            )\n",
    "        return chunks_dicts\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Embedding with SentenceTransformer\n",
   "id": "97da6e646371627c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class CustomEmbeddings:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            trust_remote_code: bool = True,\n",
    "            device: str = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "            normalize_embeddings: bool = True,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "        self.model = SentenceTransformer(\n",
    "            model_name,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = self.model.encode(\n",
    "            texts,\n",
    "            normalize_embeddings=self.normalize_embeddings,\n",
    "            convert_to_tensor=False\n",
    "        )\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        embedding = self.model.encode(\n",
    "            text,\n",
    "            normalize_embeddings=self.normalize_embeddings,\n",
    "            convert_to_tensor=False\n",
    "        )\n",
    "        return embedding.tolist()\n",
    "\n",
    "    @property\n",
    "    def embedding_dimension(self) -> int:\n",
    "        \"\"\"Get the dimension of the embeddings.\"\"\"\n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "47d639e45d606a2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Indexing & Semantic Search with LanceDB",
   "id": "25e633cdacfeb39a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "import lancedb\n",
    "from lancedb.table import Table\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "class LanceDB:\n",
    "\n",
    "    def __init__(self,\n",
    "                 vector_storage_path: str = \"./lancedb/vector_storage\",\n",
    "                 table_name: str = \"knowledge_base\"):\n",
    "        db = lancedb.connect(uri=vector_storage_path)\n",
    "        import pyarrow as pa\n",
    "        schema = pa.schema([\n",
    "            pa.field(\"content\", pa.string()),\n",
    "            pa.field(\"page_number\", pa.int32()),\n",
    "            pa.field(\"pdf_name\", pa.string()),\n",
    "            pa.field(\"embeddings\", pa.list_(pa.float32(), 1024)),\n",
    "        ])\n",
    "        try:\n",
    "            db.create_table(table_name, schema=schema)\n",
    "            print(f\"Table {table_name} created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Table {table_name} already exists. {e}\")\n",
    "        self.__table: Table = db.open_table(name=table_name)\n",
    "\n",
    "    def semantic_search(self, vector_query: list[float], n: int = 10, distance_threshold=0.50) -> DataFrame:\n",
    "        search_results = self.__table.search(vector_query, vector_column_name=\"embeddings\").distance_type(\n",
    "            \"cosine\").limit(n).to_pandas()\n",
    "        print(f\"search_results\\n\\n {search_results}\")\n",
    "        return search_results.loc[search_results[\"_distance\"] <= distance_threshold]\n",
    "\n",
    "    def get_count(self) -> int:\n",
    "        return self.__table.count_rows()\n",
    "\n",
    "    def save(self, df: DataFrame):\n",
    "        self.__table.add(df)\n",
    "        print(f\"total records in lancedb : {self.__table.count_rows()}\")\n",
    "\n",
    "    def create_index(self):\n",
    "        try:\n",
    "            self.__table.create_index(metric=\"cosine\", vector_column_name=\"embeddings\")\n",
    "        except Exception as e:\n",
    "            print(f\"Seems index already exist {e}\")\n"
   ],
   "id": "1d11124bbcd3e940",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4: Prompt Template\n",
   "id": "8a785232c63e30a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class PromptTemplate:\n",
    "    @staticmethod\n",
    "    def build(context: str, question: str, max_token: int = 512) -> str:\n",
    "        prompt = f\"\"\"You are a Climate Science Assistant using IPCC research to explain climate change clearly and compassionately.\n",
    "\n",
    "**Your Approach:**\n",
    "- Use solid IPCC scientific evidence\n",
    "- Explain concepts accessibly for all audiences\n",
    "- Be honest about uncertainties while providing clear guidance\n",
    "- Support responses with specific data and findings\n",
    "- Remain helpful, accurate, and encouraging\n",
    "- **Keep responses under {max_token} tokens**\n",
    "\n",
    "**Available Scientific Context (IPCC 2023 Synthesis Report):**\n",
    "{context}\n",
    "\n",
    "**Question:**\n",
    "{question}\n",
    "\n",
    "**Your Response (max {max_token} tokens):**\n",
    "\n",
    "        \"\"\"\n",
    "        return prompt\n"
   ],
   "id": "776867ebfa80cae9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 5: LLM Inference Using Qwen\n",
   "id": "f3ba46d5d8892c7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "class QwenLLM:\n",
    "\n",
    "    def __init__(self, model_name: str = \"Qwen/Qwen3-1.7B\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self) -> None:\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.device = self.model.device\n",
    "        print(f\"Model loaded successfully on device: {self.device}\")\n",
    "\n",
    "    def _prepare_messages(self, prompt: str) -> List[Dict[str, str]]:\n",
    "        return [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    def _parse_thinking_content(self, output_ids: List[int]) -> Tuple[str, str]:\n",
    "        try:\n",
    "            # Find the index of </think> token (151668)\n",
    "            index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            # If </think> token not found, no thinking content\n",
    "            index = 0\n",
    "        thinking_content = self.tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "        main_content = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "        return thinking_content, main_content\n",
    "\n",
    "    def invoke(self,\n",
    "               prompt: str,\n",
    "               max_new_tokens: int = 1024,\n",
    "               enable_thinking: bool = True,\n",
    "               return_thinking: bool = True,\n",
    "               **generation_kwargs) -> Dict[str, str]:\n",
    "        messages = self._prepare_messages(prompt)\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                **generation_kwargs\n",
    "            )\n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "        if enable_thinking and return_thinking:\n",
    "            thinking_content, main_content = self._parse_thinking_content(output_ids)\n",
    "            return {\n",
    "                \"response\": main_content,\n",
    "                \"thinking\": thinking_content\n",
    "            }\n",
    "        else:\n",
    "            content = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "            return {\n",
    "                \"response\": content,\n",
    "                \"thinking\": \"\"\n",
    "            }\n"
   ],
   "id": "78a28d9f81350d81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 6: Putting It All Together\n",
   "id": "6ea7f4c205722ad5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.chunker.chunker import Chunker\n",
    "from src.embedding.custom_embedding import CustomEmbeddings\n",
    "from src.llm.qwen_llm import QwenLLM\n",
    "from src.prompt.prompt_template import PromptTemplate\n",
    "from src.storage.lancedb import LanceDB\n",
    "\n",
    "pdf_data = \"https://www.ipcc.ch/report/ar6/syr/downloads/report/IPCC_AR6_SYR_LongerReport.pdf\"\n",
    "\n",
    "EMBEDDING_MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "LLM_MODEL = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "\n",
    "# initialize the embedding model\n",
    "embeddings = CustomEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "# initialize the LLM\n",
    "llm = QwenLLM(model_name=LLM_MODEL)\n",
    "\n",
    "# initialize the Chunker\n",
    "chunker = Chunker(embedding_model=EMBEDDING_MODEL)\n",
    "\n",
    "# initialize the Vector DB\n",
    "lancedb = LanceDB(table_name=\"rag_table\")\n",
    "# Run document Indexing\n",
    "print(\"Start Chunking ....\")\n",
    "documents = chunker.chunk(pdf_data)\n",
    "print(\"Chunking done....\")\n",
    "df = pd.DataFrame(documents, columns=[\"content\", \"page_number\", \"pdf_name\"])\n",
    "print(\"Start Embedding ....\")\n",
    "df[\"embeddings\"] = df[\"content\"].apply(embeddings.embed_query)\n",
    "print(\"Embedding  done....\")\n",
    "print(df)\n",
    "print(\"Start saving ....\")\n",
    "lancedb.save(df)\n",
    "\n",
    "# RAG\n",
    "query = \"How is climate change affecting biodiversity?\"\n",
    "\n",
    "vector_query = embeddings.embed_query(query)\n",
    "result_df = lancedb.semantic_search(vector_query=vector_query, n=2)\n",
    "context = \"\\n\\n\".join(result_df[\"content\"].tolist())\n",
    "formatted_prompt = PromptTemplate.build(context=context, question=query)\n",
    "print(\"\\nFormatted Prompt:\" + \"\\n\" + formatted_prompt)\n",
    "final_response = llm.invoke(formatted_prompt, enable_thinking=True, return_thinking=True)\n",
    "print(\"\\nFinal RAG Response:\")\n",
    "print(final_response[\"response\"])\n"
   ],
   "id": "b817a1f699870e1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3170f7fb93969a55",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
